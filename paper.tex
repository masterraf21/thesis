\documentclass[conference]{configs/IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multirow}
%\def\tablename{Table}
%\def \figurename{Figure}

%\usepackage[table,xcdraw]{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Development of Performance Regression Analysis Tool using Distributed Tracing on Microservice-based Application\\
%{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and should not be used}
%\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{Rafi Abbel Mohammad}
\IEEEauthorblockA{\textit{School of Electrical Engineering and Informatics} \\
\textit{Institut Teknologi Bandung}\\
Bandung, Indonesia \\
masterraf21@gmail.com}
%\and
%\IEEEauthorblockN{Achmad Imam Kistijantoro}
%\IEEEauthorblockA{\textit{School of Electrical Engineering and Informatics} \\
%\textit{ITB, Indonesia}\\
%Bandung, Indonesia \\
%imam@stei.itb.ac.id}
}

\maketitle

\begin{abstract}
In applications with Microservice architecture, when a significant change occurs and result in decreased performance or regression, it is difficult to perform analysis to check which part of the whole Microservice is the main cause of the regression due to the distributed nature of Microservice. Distributed tracing can be used to analyze and determine whether there is a regression and the cause of the regression by utilizing latency data from existing operations on Microservice application.
This system will perform performance regression analysis using the distributed tracing tool Zipkin. Regression is then can be detected using Kolmogorov-Smirnov statistical analysis which will compare the latency data samples that occur
periodically and the sample latency baseline data that represents application performance  under normal circumstances. The analysis will compare the Cummulative
Distribution Function (CDF) of the two samples and see if both
CDF comes from a different distribution. If it is found that both CDFs originate from different distributions, it then can be suspected that there has been a performance regression because the distribution of the periodic data has deviated from the distribution of the baseline data.
If a regression is detected, the system will then perform a critical path analysis to see which operations of each services are most likely contributed to the regression. The analysis will be carried out by finding the difference
latency of periodic and baseline operation data and it will be seen which operation's latency difference exceeds a predetermined limit.
The performance regression analysis system has been tested on a Microservice application that runs on Kubernetes. The result is that out of 10 out of 11 test cases, and for every successful case, the main suspected operation that causes regression is found. Implementing the system adds an average overhead of 0.78\% CPU usage and 0.67\% Memory usage.
\end{abstract}

\begin{IEEEkeywords}
performance regression, Distributed Tracing, Microservice, Kubernetes
\end{IEEEkeywords}

\section{Introduction}
\label{1}
With the widespread adoption use of Microservice architecture based on distributed systems today, more and more challenges arise related to the adoption of this architectural style. Microservice architecture offers several advantages, including technological heterogeneity, resilience, ease of \textit{scaling}, ease of \textit{deployment}, ease of alignment with a technology team, flexibility in determining application composition, and an optimized system to replace components with each other \cite{building-microservices}. These advantages make the adoption of the Microservice architecture quite widespread, especially in applications that require \textit{scalability} to serve the growing needs of customers.

However, with the many advantages offered by the Microservice architecture, it will also increase the complexity to perform analysis when there is a decline in performance or regression in the application. This is due to the distributed nature of Microservice by dividing the application into smaller services, so to find which operation of the services causing the regression will be difficult and will be inefficient to perform analysis on each and every available services, especially if the number of the services has reached hundreds or thousands.

There is already a tool that can help developers to perform monitoring on distributed systems such as Microservice, which is distributed tracing. With the help of trace, developers can get an overview of each request that occurs in a resource or component that interacts with other components in a distributed system such as node, service, network, or mutex. The traces then can be processed for various purposes, such as creating a dependency map between services, perform a performance analysis of the service, and also perform a performance regression analysis using the latency data from the trace result.

We provide the implementation of the performance regression analysis (PRA) system using the distributed tracing tool Zipkin which is available on Github\footnote{\url{https://github.com/masterraf21/pra\textunderscore engine}}. The PRA system will perform detection on the Microservice application when a regression occurs and perform analysis to determine the main cause of the regression. Performance regression analysis will be done by using the help of the trace results from Zipkin which will be able to help developers restore performance of Microservice application after reggresion occurs.

The paper is organized as follows. Section II presents related
work in the area of performance regression. Section III discusses problem analysis, the design and the underlying architecture, and the implementation of the PRA system. Section IV shows the experiments done with the PRA system. Section V discusses a few analysis from the experiments done in Section V. Section
VI concludes the work.


\section{Related Works}
This section will discuss two related works in the field of performance regression.

\subsection{Performance Regression Detection in DevOps}
This study \cite{regression-detection} proposes to perform regression detection performance at the DevOps stage, i.e. at the level of the source code change. The main thing considered is to avoid detecting performance regressions after the app is deployed and running in a production environment. There are two ways proposed by the research
to detect performance regressions at the DevOps stage:
\begin{enumerate}
	\item Perform regression detection at the source code level by extracting
	every Git commit and identify which test cases are affected by
	regression on each commit. After that, the performance of each commit will be
	evaluated chronologically using the relevant test cases or
	benchmarks of performance. Finally, a statistical analysis will be carried out on the performance evaluation
	to identify which changes resulted in the performance regression. To find out the root of the problem at the code level, an iterative approach will be done  to identify the root causes of
	regression until it can no longer generate new reasons.
	\item Predict which tests may manifest regression at the commit level. This approach is a continuation of the previous point where
	the approach of doing a test on every commit takes a lot
	resources and effort so that this approach will make predictions
	automatically whether a test will manifest a performance regression given a commit code. In detail, this approach will detect whether there is a regression on certain tests of a commit. To make a prediction model, it must first identified which tests are prone to yielding regression by evaluating all the tests affected by the code changes in previous commits by calculating the performance of each test, such as response time,
	CPU and Memory usage, I/O reads and writes, by doing
	each test sequentially.
\end{enumerate}

\subsection{A Real-time Trace-level Root-cause Diagnosis System in Alibaba Datacenters}
This study \cite{rca-alibaba} uses distributed tracing to perform
Diagnostics of performance degradation on a system by performing graph analysis on
trace level in real-time to determine the root cause of performance degradation or
root cause. This approach was developed based on performance profiling, tracking, and logging system in Alibaba datacenter to speed up root cause diagnosis in real-time. The developed system allows the discovery of normative patterns and corresponding key graph properties, which are saved and updated offline as a knowledge base for further use in risk estimation at the trace level and identify transitions that are unexpected deviations from the normal
normal as shown in Fig. \ref{rca-framwork}.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.5\textwidth]{resources/ch2/rca-framework.png}
	\caption{Workflow of Root-cause Analysis on \cite{rca-alibaba}}
	\label{rca-framwork}
\end{figure}

\section{Proposed Solution}
This section explains analysis of the problem introduced in section \ref{1}, the general solution to solve the problem, the architecture, and the implementation of the system.

\subsection{Problem Analysis}
As the complexity of Microservice applications increases, such as with the increasing number of servce and the connectivity between existing servces therein, there will also be an increase in the complexity of monitoring the performance on the entire Microservice application. One of the important issues related to performance is a regression or decrease in performance. The need for urgently determining the main cause of the regression in the system becomes important if the regression occurs in a production environment that directly serves requests from customers, so that if it is not addressed immediately it will have a direct impact on the
user experience in using the application.

There are two approaches that can be taken to solve the problem of performance regression \cite{regression-detection}, namely:
\begin{enumerate}
	\item Regression detection is carried out after the application has been developed and deployed in a dedicated environment:
	\item Regression detection is carried out before the application is developed and deployed, and then conducting a study of the resulting changes to the source code
\end{enumerate} 

Given the distributed nature of Microservices, it will be difficult for developers if regression detection is carried out with an individual approach on each existing servce especially when the number of existing servce is very large, and the interdependence relationship between services becomes complicated. Therefore, to overcome performance regression in Microservice-based application, an approach that does not require developers to source search is needed. So that in the case of detection and regression analysis of Microservice-based application performance, the first approach is more suitable to use.

Therefore, the workflow that must be carried out by the performance regression analysis (PRA) system is as follows:
\begin{enumerate}
	\item The system must be able to detect when there is a regression in Microservice-based application
	\item The system must be able to determine the source or root cause of the regression after it is detected
\end{enumerate}

Seeing the needs of the PRA system above, the approach the will be carried out to do performance regression analysis will be Cummulative Analysis and Aggregation Analysis. 

Cummulative Analysis is an approach to analyze certain metrics of application such as latency by cummulating all the collected data as a single representation such as Histogram. With this approach, the distributed nature of many services in Microservice-based application can be observed as a single figure or representation so that the whole Microservice application can be monitored in a single space. One of the available tool to analyze and compare large sets of data is called Kolmogorov-Smirnov (K-S) statistic test. The K-S statistic test will count the difference between two Cummulative Distribution Functions (CDF) as a scalar number \cite{kolmogorov_1951}.

K-S statistic are obtained by finding the furthest vertical distance between the two CDFs as indicated by the arrows. The greater the distance, the more likely it is that two CDFs are from two different distributions so that it can be an indication that there has been a change on performance.
\begin{figure}[htb]
	\centering
	\includegraphics[width=0.35\textwidth]{resources/ch2/ks.png}
	\caption{CDF of two samples, arrows indicate the Kolmogorov-Smirnov statistical value \cite{wiki:ks-test}}
	\label{ks-example}
\end{figure}

We first define two hypotheses, $h_{0}$ or the null hypothesis which states that the two data samples come from the same distribution and $h_{a}$ or an alternative hypothesis which states that both sample data comes from two different distributions.

The K-S test will produce two values, \texttt{statistic} and \texttt{pvalue}. If the \texttt{pvalue} is less than the significant value, or alpha which can be selected from a certain constants,
then we can reject $h_{0}$ and accept $h_{a}$. If the \texttt{pvalue} is not less than significant value, then the next step is to compare the statistical value and critical value. Critical value itself is a point on the hypothesis that will be compared to the results of the statistical values to be used to reject or accept the null hypothesis. If the statistical value is greater than critical value, then we can reject $h_{0}$ and accept $h_{a}$ or meaning that the two data samples come from different distributions. However, if the statistical value is smaller than the critical value, we cannot reject $h_{0}$ or it means both sample data come from the same distribution.

The Cummulative Analysis approach will be used to fullfil the first needs of the PRA system by using Kolmogorov-Smirnov statistic test.

On the other hand, Aggregation Analysis can help to identify changes that are not too visible on the performance of a service. When comparing two sets of traces, we can also see the latency contribution associated with the tag,
operations, and services and also see how these contributions change over time time. Average contribution metric
such as the latency of each service and its operation will be determined from each set. Then the mean difference of the contribution metrics from the baseline and regression sets
will be calculated and sorted so that it can be seen which operation or service is actually the root cause of the regression.

Tab. \ref{aggregate} shows the result of the Aggregation Analysis approach which will be useful in determining the possible root cause of the regression. This approach will be used to fullfil the second needs of the PRA system.
\begin{table}[!htb]
	\caption{Aggregation Analysis result}
	\label{aggregate}
	\centering
	\begin{tabular}{|l|l|l|l|}
		\hline
		\textbf{Service/Operation} & \textbf{Baseline} & \textbf{Regression} & \textbf{Difference} \\ \hline
		inventory/write-cache & 63.1 ms & 368 ms  & +305 ms  \\ \hline
		inventory-db/update   & 1.75 ms & 2.26 ms & +0.51 ms \\ \hline
		memcached/set         & 4.94 ms & 4.71 ms & -0.23 ms \\ \hline
		inventory/update      & 15.2 ms & 14.8 ms & -0.47 ms \\ \hline
		inventory/db-update   & 32 ms   & 30.6 ms & -1.4 ms  \\ \hline
	\end{tabular}
\end{table}

Fig. \ref{flow-pra} illustrates the phases to be carried out by the PRA system. In general, there will be two phases to be carried out, namely the baseline loading phase
and the periodic analysis phase. The first phase, namely the loading phase, will look for stable application activity data that can be used as a baseline or a reference for the analysis to be carried out in the next stages. This phase
will produce two artifacts, the latency data of all recorded traces by the tracing system, and operating data that occurs on the critical path of all traces along with its latency value which will be stored as a key-value pair as well.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth,scale=5]{resources/ch3/alur_v2.png}
	\caption{PRA system workflow}
	\label{flow-pra}
\end{figure}

The next phase is the periodical analysis phase. This phase will run continuously and the analysis will be run periodically at intervals of every 5 minutes. This is carried out to continuously detect whether there is a regression and
immediately determine the source of the regression if it occurs. There are several stages of analysis
to be carried out in this phase. The first analysis stage will compare the results transformation of data latency of all ongoing traces periodically with the baseline latency artifact from the loading phase. The comparison will be made within a certain period. The comparison of the two latency data will produce Kolmogorov-Smirnov statistic. If the Kolmogorov-Smirnov statistic that is generated indicates that the two samples of latency data come from two different distributions, then it can be an indication that there has been a regression in the data capture in the periodical phase.

If a regression is detected, the next step is to perform a critical path analysis by calculating the contribution of the critical path data trace from the regression period and will be compared with the critical path data derived from the baseline calculation. From this comparison, it will be sorted and the result of which operation has the largest latency difference becomes strong candidate as the root cause of the regression.

\subsection{Architecture}
Fig. \ref{arch-pra} shows the architecture of the PRA system. There will be only two components that will be implemented from scratch, which are the PRA Engine and the PRA UI. The rest of the components are already present and ready to use from Zipkin. The PRA Engine will fetch data from Zipkin API to retrieve the traces generated and will implement the logic from Fig \ref{flow-pra}. The PRA Engine will be exposed as an Application Programming Interface (API). The PRA UI will fetch data from the PRA Engine API and will present the state of regression in the application, and the critical path analysis result.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.4\textwidth]{resources/ch3/arch.png}
	\caption{PRA system architecture}
	\label{arch-pra}
\end{figure}

\subsection{Implementation}
Implementation of the performance regression analysis (PRA) system will be made based on the architectural design as shown in Fig. \ref{arch-pra}. Component
like the instrumentation library instead will use the available
Open Source distributed tracing from Zipkin. Components to be made from scratch entirely are the PRA User Interface (UI) and PRA Engine components of the system.

Before implementing the solution, we  must first
select the tools that will be used to do the implementation of the PRA Engine. In general, there are many programming languages that can implement solutions of the PRA Engine. However, there is an important need to implement the Kolmogorov-Smirnov (K-S) statistical test which not all programming languages have a support library to do this. There are two programming languages that have libraries for performing K-S tests
which are the Go and Python programming languages. The Go language has the Gonum\footnote{\url{gonum.org}} library which
has a K-S test implementation in the \texttt{KolmogorovSmirnov} function, while the language
Python has the SciPy\footnote{\url{scipy.org}} library which has a K-S test implementation in the \texttt{scipy.stat.ks\textunderscore 2samp} function. From the two languages, we finally decide to choose Python language to implement the solution after successfully parsed the Zipkin trace data with Object-oriented Programming approach based on source code from Zipkin's User Interface application which uses the Javascript language.

\section{Experiment}
We test the PRA system in the context of \textbf{Hipster Shop} application hosted by Google Kubernetes Engine, Container Optimized OS (COS), in 3 nodes of n1-standard-2 machine with 2 vCPU and 7.5 GiB RAM.

\subsection{Hipster Shop}
Hipster Shop\footnote{\url{https://github.com/masterraf21/microservices-demo}} is a cloud native microservices demo application
where users can browse items, add them to the
cart, and purchase them. The application contains a 10-tier
microservices application that communicates using gRPC.
Hipster Shop also provides a load generator to simulate user
behaviour. For this experiment purpose, the version of the Hipster Shop which will be used is already instrumented to work with the Zipkin distributed tracing.

\subsection{Experiment Method}
To perform experiment on the PRA system that has been created, the existing services in the Hipster Shop will be modified to imitate the behavior service that experiences performance regression. There are two main methods that can be used to create such behavior which are adding a sleep command and adding a loop with dereference operation of a variable which requires a lot of CPU power to solve it so that it is expected that the functions will have more latency.

There are two stages that will be carried out to do this experiment test, the first is baseline data collection stage. Baseline data will be taken by simulating
the usage of the application by using the Load Generator which will run with variable of \textbf{25 Users} and \textbf{5 Spawn Rates} for \textbf{1 hour}. This baseline data will then be stored by the engine for use in future test cases. The next step is to carry out experiment testing with cases which will be explained in more detail further.

There are two types of test cases to be tested on the system. Tirst type are cases that simulate regression by making changes that occurs at the application level by making modifications intentionally at the code level.
The first case will have an ID with an \textbf{SI} prefix signifying the \textit{Scenario-Internal}.
The second type are cases that simulate regression by increasing the number of users and load on the Load Generator. The second case will have an ID with the \textbf{SE} prefix denotes an \textit{Scenario-External}. Tab. \ref{testcases} will explain the cases
system testing.

For testing purposes, 3 significant levels or alpha will be used, those are 0.1, 0.05, and 0.001. These three significant levels are used for knowing the sensitivity of the regression detection results with interpretation that the smaller the value of
significant levels, the more significant the evidence to reject $h_{0}$. Significant value 0.1
was chosen as the upper limit of the significant value to determine the sensitivity level of test case with the greatest significant value. While the value of 0.05 was chosen because this value is the most commonly used value as a significant value on the statistical test of the hypothesis. While the value of 0.001 was chosen as a comparison of the value of least significant to test the lower limit of the significant value.

\subsection{Experiment Result}
The following tables \ref{test-1} \ref{test-2} \ref{test-3} describe the summaries of the experiment results that have been performed with three different alpha values. The result is on all test cases, IS2 cases to IS4 cases were successfully detected by regression and critical path analysis is appropriate. It's just that for the SI1 case, only the alpha value is 0.1
which can successfully detect the regression and determine the source of the cause of the regression with critical path analysis. Other alpha values cannot detect regression and do not perform critical path analysis.

%\pagebreak
\begin{table}[htb]
	\caption{Test Cases}
	\label{testcases}
%	\centering
	\begin{tabular}{|l|l|c|c|l|}
		\hline
		\textbf{ID} &
		\textbf{Service/Function} &
		\multicolumn{1}{l|}{\textbf{\begin{tabular}[c]{@{}l@{}}Extra \\ Latency\end{tabular}}} &
		\multicolumn{1}{l|}{\textbf{User}} &
		\textbf{Description} \\ \hline
		IS1 &
		CheckoutService/PlaceOrder &
		100ms &
		25 &
		Extra latency case \#1 \\ \hline
		IS2 &
		CheckoutService/PlaceOrder &
		250ms &
		25 &
		Extra latency case \#2 \\ \hline
		IS3 &
		CheckoutService/PlaceOrder &
		350ms &
		25 &
		Extra latency case \#3 \\ \hline
		IS4 &
		CheckoutService/PlaceOrder &
		250ms &
		75 &
		\begin{tabular}[c]{@{}l@{}}Extra latency with\\ increased user\end{tabular} \\ \hline
		IS5 &
		CheckoutService/PlaceOrder &
		250ms &
		150 &
		\begin{tabular}[c]{@{}l@{}}Extra latency with\\ increased user\end{tabular} \\ \hline
		IS6 &
		CheckoutService/PlaceOrder &
		- &
		25 &
		Extra CPU case \\ \hline
		\multirow{5}{*}{IS7} &
		CheckoutService/PlaceOrder &
		\multirow{5}{*}{250ms} &
		\multirow{5}{*}{25} &
		\multirow{5}{*}{\begin{tabular}[c]{@{}l@{}}Extra latency case\\ with multiple \\ services\end{tabular}} \\ \cline{2-2}
		&
		ProductCatalog/GetProduct &
		&
		&
		\\ \cline{2-2}
		&
		ProductCatalog/ListProducts &
		&
		&
		\\ \cline{2-2}
		&
		ShippingService/GetQuote &
		&
		&
		\\ \cline{2-2}
		&
		\begin{tabular}[c]{@{}l@{}}RecommendationService/\\ ListRecommendations\end{tabular} &
		&
		&
		\\ \hline
		ES1 &
		All Normal &
		- &
		75 &
		\begin{tabular}[c]{@{}l@{}}No extra latency\\ with increased user \#1\end{tabular} \\ \hline
		ES2 &
		All Normal &
		- &
		150 &
		\begin{tabular}[c]{@{}l@{}}No extra latency\\ with increased user \#2\end{tabular} \\ \hline
		ES3 &
		\begin{tabular}[c]{@{}l@{}}Checkout load\\ increased to 100 in Loadgen\end{tabular} &
		- &
		25 &
		\begin{tabular}[c]{@{}l@{}}Increased external\\ load case \#1\end{tabular} \\ \hline
		ES4 &
		\begin{tabular}[c]{@{}l@{}}Curency load \\ increased to 100 in Loadgen\end{tabular} &
		- &
		25 &
		\begin{tabular}[c]{@{}l@{}}Increased external\\ load case \#2\end{tabular} \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!htb]
	\caption{Experiment Result for Alpha of 0.1}
	\label{test-1}
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{ID} & \multicolumn{1}{l|}{\textbf{Regression Detected}} & \multicolumn{1}{l|}{\textbf{Critical Path Analysis Correct}} \\ \hline
		IS1 & Yes & Yes \\ \hline
		IS2 & Yes & Yes \\ \hline
		IS3 & Yes & Yes \\ \hline
		IS4 & Yes & Yes \\ \hline
		IS5 & Yes & Yes \\ \hline
		IS6 & Yes & Yes \\ \hline
		IS7 & Yes & Yes \\ \hline
		ES1 & Yes & Yes \\ \hline
		ES2 & Yes & Yes \\ \hline
		ES3 & Yes & Yes \\ \hline
		ES4 & Yes & Yes \\ \hline
	\end{tabular}
\end{table}

\begin{table}[!htb]
	\caption{Experiment Result for Alpha of 0.05}
	\label{test-2}
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{ID} & \multicolumn{1}{l|}{\textbf{Regression Detected}} & \multicolumn{1}{l|}{\textbf{Critical Path Analysis Correct}} \\ \hline
		IS1 & No & No \\ \hline
		IS2 & Yes & Yes \\ \hline
		IS3 & Yes & Yes \\ \hline
		IS4 & Yes & Yes \\ \hline
		IS5 & Yes & Yes \\ \hline
		IS6 & Yes & Yes \\ \hline
		IS7 & Yes & Yes \\ \hline
		ES1 & Yes & Yes \\ \hline
		ES2 & Yes & Yes \\ \hline
		ES3 & Yes & Yes \\ \hline
		ES4 & Yes & Yes \\ \hline
	\end{tabular}
\end{table}
\begin{table}[!htb]
	\caption{Experiment Result for Alpha of 0.001}
	\label{test-3}
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{ID} & \multicolumn{1}{l|}{\textbf{Regression Detected}} & \multicolumn{1}{l|}{\textbf{Critical Path Analysis Correct}} \\ \hline
		IS1 & No & No \\ \hline
		IS2 & Yes & Yes \\ \hline
		IS3 & Yes & Yes \\ \hline
		IS4 & Yes & Yes \\ \hline
		IS5 & Yes & Yes \\ \hline
		IS6 & Yes & Yes \\ \hline
		IS7 & Yes & Yes \\ \hline
		ES1 & Yes & Yes \\ \hline
		ES2 & Yes & Yes \\ \hline
		ES3 & Yes & Yes \\ \hline
		ES4 & Yes & Yes \\ \hline
	\end{tabular}
\end{table}

\pagebreak

\subsection{Overhead Testing}
Of the 11 test cases, the average PRA Engine tool uses 0.046
vCPU with standard deviation of 0.0038 and 154.41 MiB Memory with standard
deviation of 17.91 in one analysis. The entire cluster has resources
as much as 6 vCPUs and 22.5 GiB, so that in one time doing PRA Engine analysis
resulting in CPU overhead as much as 0.78\% and Memory as much as 0.67\% to the Kubernetes cluster. Overhead caused by the tool is low so that the use of PRA Engine tools will have no impact on the overall performance of the Kubernetes cluster. Tab. \ref{overhead-tbl} shows the result of the overhead testing.
\begin{table}[!htb]
	\caption{Overhead testing result}
	\label{overhead-tbl}
	\centering
	\begin{tabular}{|l|c|c|}
		\hline
		\multicolumn{1}{|l|}{ID} & \multicolumn{1}{l|}{CPU (vCPU)} & \multicolumn{1}{l|}{Memory (MiB)} \\ \hline
		\textbf{IS1}     & 0,042 & 137,82  \\ \hline
		\textbf{IS2}     & 0,043 & 128,71  \\ \hline
		\textbf{IS3}     & 0,04  & 131,55  \\ \hline
		\textbf{IS4}     & 0,045 & 146,6   \\ \hline
		\textbf{IS5}     & 0,046 & 153,61  \\ \hline
		\textbf{IS6}     & 0,047 & 152,71  \\ \hline
		\textbf{IS7}     & 0,048 & 152,71  \\ \hline
		\textbf{ES1}     & 0,049 & 168,8   \\ \hline
		\textbf{ES2}     & 0,05  & 183,68  \\ \hline
		\textbf{ES3}     & 0,051 & 176,59  \\ \hline
		\textbf{ES4}     & 0,052 & 165,75  \\ \hline
		\textbf{Average} & 0,046 & 154,411 \\ \hline
		\textbf{STD}     & 0,038 & 17,916  \\ \hline
	\end{tabular}
\end{table}
%\pagebreak

\section{Analysis}
Of all the test cases, the results obtained from the test will be
depending on the significant value used to test each case.
There are 3 significant values that the author uses to perform the test, namely 0.1, 0.005, and 0.001 with the aim of testing the sensitivity of the detection algorithm regressions are given different significant values. The result is a test with
a significant value of 0.1 successfully detects and analyzes the cause of the regression with true in all test cases, but in tests with a significant value of 0.05 and 0.001 SI1 cases were not detected as regression.

Fig. \ref{result_log_1} shows the log obtained from the IS1 case with a significant value 0.05. It can be seen that the \texttt{pvalue} generated from the test is 0.067. On the test, the significant value used is 0.05 so that based on algorithm, in that case it cannot immediately reject $h_{0}$ and must continue compare the statistical value with the critical value, and the result states that the CDF latency in these cases has the same distribution with the baseline, so that regression is not detected. The same will apply to all significant values $<0.05$, but in the test with a significant value of 0.1,
the IS1 case will be detected as regression.
\begin{figure}[!htb]
	\centering
	\includegraphics[width=0.5\textwidth]{resources/ch4/log/1-log-new.png}
	\caption{\textit{Log} hasil pengujian kasus SI1 dengan alpha 0,05}
	\label{result_log_1}
\end{figure}

From the analyis above, it can be interpreted that the use of significant values on the PRA Engine will have an effect on the results, especially if the regression case to be detected does not
has a very significant change in its latency value as shown in
in the case of IS1. It can be concluded that the greater the significant value, the more sensitive the algorithm is in detecting regressions that occur because they don't
very strong evidence to reject $h_{0}$ but it means that $h_{0}$ cannot be rejected too confidently. Conversely, the smaller the significant value used, the smaller the value, the stronger the evidence needed to reject $h_{0}$ which means that $h_{0}$ can be rejected with more confidence.

In the case of other internal change factors other than IS1, all test results
with three significant values can be successfully detected and the source can be determined because the algorithm can determine that the CDF data latency case regression
has a different distribution with the CDF baseline latency data. While on cases that simulate changes from external, i.e. cases ES1 to
ES4, PRA Engine can also detect the occurrence of regression but the results of critical path analysis cannot determine the source of the regression because it does not caused by changes in service level.

It can be concluded that the algorithm has not been able to determine whether the regression
occurs due to changes that occur at the application level or changes
that occur outside the application such as an increase in the number of users or an increase in
load on some operations. However, if from the critical path analysis result can be seen the difference
latency of operations that exceed a certain threshold can be an indication that
regression that occurs is caused by changes at the application level. On the other hand, if
critical path analysis cannot determine which operation is a candidate that causes
regression because the difference is not greater than the threshold, it can be and
indication that the detected regression is due to changes caused by
by external factors.

\section{Conclusion \& Future Works}
The implemented performance regression analysis system can detect and analyze the cause of regression on Microservice-based application using Kolmogorov-Smirnov test and Critical Path analysis. However, the Kolmogorov-Smirnov test alone cannot determine exactly whether the detected regression is caused by internal or external factors, therefore the Critical Path analysis is required to help determine the exact cause of the regression.

In the future, a similar system can be implemented to analyze performance regression outside of Kubernetes environment.

%\section*{Acknowledgment}
%
%The preferred spelling of the word ``acknowledgment'' in America is without 
%an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
%G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
%acknowledgments in the unnumbered footnote on the first page.

\bibliographystyle{configs/IEEEtran}
\bibliography{references}

\vspace{12pt}


\end{document}
